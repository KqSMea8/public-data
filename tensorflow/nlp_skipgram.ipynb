{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/backstopmedia/tensorflowbook/tree/master/chapters/\n",
    "# 06_recurrent_neural_networks_and_natural_language_processing/01_wikipedia\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import lazy_property\n",
    "from helpers import AttrDict\n",
    "\n",
    "# 1.下载转储文件，提取词语\n",
    "# 2.统计词语的出现次数，构建一个由最常见词汇构成的词汇表  只存最常见词汇？\n",
    "# 3.利用词汇表对页面编码\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "from helpers import download\n",
    "\n",
    "# corpus = Wikipedia(\n",
    "#     'https://dumps.wikimedia.org/enwiki/20160501/'\n",
    "#     'enwiki-20160501-pages-meta-current1.xml-p000000010p000030303.bz2',\n",
    "#     WIKI_DOWNLOAD_DIR,\n",
    "#     params.vocabulary_size)\n",
    "\n",
    "class Wikipedia:\n",
    "\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "\n",
    "    def __init__(self, url, cache_dir, vocabulary_size=10000):\n",
    "        self._cache_dir = os.path.expanduser(cache_dir)\n",
    "        self._pages_path = os.path.join(self._cache_dir, 'pages.bz2')\n",
    "        self._vocabulary_path = os.path.join(self._cache_dir, 'vocabulary.bz2')\n",
    "        if not os.path.isfile(self._pages_path):\n",
    "            print('Read pages')\n",
    "            self._read_pages(url)\n",
    "        if not os.path.isfile(self._vocabulary_path):\n",
    "            print('Build vocabulary')\n",
    "            self._build_vocabulary(vocabulary_size)\n",
    "        with bz2.open(self._vocabulary_path, 'rt') as vocabulary:\n",
    "            print('Read vocabulary')\n",
    "            self._vocabulary = [x.strip() for x in vocabulary]\n",
    "        self._indices = {x: i for i, x in enumerate(self._vocabulary)}\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over pages represented as lists of word indices.\"\"\"\n",
    "        with bz2.open(self._pages_path, 'rt') as pages:\n",
    "            for page in pages:\n",
    "                words = page.strip().split()\n",
    "                words = [self.encode(x) for x in words]\n",
    "                yield words\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return len(self._vocabulary)\n",
    "\n",
    "    def encode(self, word):\n",
    "        \"\"\"Get the vocabulary index of a string word.\"\"\"\n",
    "        return self._indices.get(word, 0)\n",
    "\n",
    "    def decode(self, index):\n",
    "        \"\"\"Get back the string word from a vocabulary index.\"\"\"\n",
    "        return self._vocabulary[index]\n",
    "\n",
    "    def _read_pages(self, url):\n",
    "        \"\"\"\n",
    "        Extract plain words from a Wikipedia dump and store them to the pages\n",
    "        file. Each page will be a line with words separated by spaces.\n",
    "        \"\"\"\n",
    "        wikipedia_path = download(url, self._cache_dir)\n",
    "        with bz2.open(wikipedia_path) as wikipedia, \\\n",
    "                bz2.open(self._pages_path, 'wt') as pages:\n",
    "            for _, element in etree.iterparse(wikipedia, tag='{*}page'):\n",
    "                if element.find('./{*}redirect') is not None:\n",
    "                    continue\n",
    "                page = element.findtext('./{*}revision/{*}text')\n",
    "                words = self._tokenize(page)\n",
    "                pages.write(' '.join(words) + '\\n')\n",
    "                element.clear()\n",
    "\n",
    "    def _build_vocabulary(self, vocabulary_size):\n",
    "        \"\"\"\n",
    "        Count words in the pages file and write a list of the most frequent\n",
    "        words to the vocabulary file.\n",
    "        \"\"\"\n",
    "        counter = collections.Counter()\n",
    "        with bz2.open(self._pages_path, 'rt') as pages:\n",
    "            for page in pages:\n",
    "                words = page.strip().split()\n",
    "                counter.update(words)\n",
    "        common = ['<unk>'] + counter.most_common(vocabulary_size - 1)\n",
    "        common = [x[0] for x in common]\n",
    "        with bz2.open(self._vocabulary_path, 'wt') as vocabulary:\n",
    "            for word in common:\n",
    "                vocabulary.write(word + '\\n')\n",
    "\n",
    "    @classmethod\n",
    "    def _tokenize(cls, page):\n",
    "        words = cls.TOKEN_REGEX.findall(page)\n",
    "        words = [x.lower() for x in words]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterator, batch_size):\n",
    "    \"\"\"Group a numerical stream into batches and yield them as Numpy arrays.\"\"\"\n",
    "    while True:\n",
    "        data = np.zeros(batch_size)\n",
    "        target = np.zeros(batch_size)\n",
    "        for index in range(batch_size):\n",
    "            data[index], target[index] = next(iterator)\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgrams(pages, max_context):\n",
    "    \"\"\"Form training pairs according to the skip-gram model.\"\"\"\n",
    "    for words in pages:\n",
    "        for index, current in enumerate(words):\n",
    "            context = random.randint(1, max_context)\n",
    "            for target in words[max(0, index - context): index]:\n",
    "                yield current, target\n",
    "            for target in words[index + 1: index + context + 1]:\n",
    "                yield current, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel: \n",
    "    def __init__(self, data, target, params):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.params = params\n",
    "        self.embeddings\n",
    "        self.cost\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def embeddings(self):\n",
    "        initial = tf.random_uniform(\n",
    "            [self.params.vocabulary_size, self.params.embedding_size],\n",
    "            -1.0, 1.0)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            self.params.learning_rate, self.params.momentum)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        embedded = tf.nn.embedding_lookup(self.embeddings, self.data)\n",
    "        weight = tf.Variable(tf.truncated_normal(\n",
    "            [self.params.vocabulary_size, self.params.embedding_size],\n",
    "            stddev=1.0 / self.params.embedding_size ** 0.5))\n",
    "        bias = tf.Variable(tf.zeros([self.params.vocabulary_size]))\n",
    "        target = tf.expand_dims(self.target, 1)\n",
    "        return tf.reduce_mean(tf.nn.nce_loss(\n",
    "            weight, bias, embedded, target,\n",
    "            self.params.contrastive_examples,\n",
    "            self.params.vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_DOWNLOAD_DIR = './data/wikipedia'\n",
    "\n",
    "params = AttrDict(\n",
    "    vocabulary_size=10000,\n",
    "    max_context=10,\n",
    "    embedding_size=200,\n",
    "    contrastive_examples=100,\n",
    "    learning_rate=0.5,\n",
    "    momentum=0.5,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "data = tf.placeholder(tf.int32, [None])\n",
    "target = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "model = EmbeddingModel(data, target, params)\n",
    "\n",
    "corpus = Wikipedia(\n",
    "    'https://dumps.wikimedia.org/enwiki/20160501/'\n",
    "    'enwiki-20160501-pages-meta-current1.xml-p000000010p000030303.bz2',\n",
    "    WIKI_DOWNLOAD_DIR,\n",
    "    params.vocabulary_size)\n",
    "\n",
    "examples = skipgrams(corpus, params.max_context)\n",
    "batches = batched(examples, params.batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "average = collections.deque(maxlen=100)\n",
    "for index, batch in enumerate(batches):\n",
    "    feed_dict = {data: batch[0], target: batch[1]}\n",
    "    cost, _ = sess.run([model.cost, model.optimize], feed_dict)\n",
    "    average.append(cost)\n",
    "    print('{}: {:5.1f}'.format(index + 1, sum(average) / len(average)))\n",
    "    if index > 100000:\n",
    "        break\n",
    "\n",
    "embeddings = sess.run(model.embeddings)\n",
    "np.save(WIKI_DOWNLOAD_DIR + '/embeddings.npy', embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
