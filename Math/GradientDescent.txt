梯度下降 （Gradient Descent）
http://www.cnblogs.com/pinard/p/5970503.html

函数在某个点的位置法向量

无约束优化问题
* 梯度下降 需要选择步长，迭代求解
    。批量梯度
    。随机梯度
* 最小二乘法 计算解析解 样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快
* 牛顿法&拟牛顿法 迭代求解，用二阶的海森矩阵的逆矩阵或伪逆矩阵求解，法收敛更快，每次迭代的时间比梯度下降法长
在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。
梯度向量,函数变化增加最快的地方

梯度下降-求解损失函数的最小值
梯度上升-求解损失函数的最大值

局部最优解 - 全局最优解

损失函数，度量拟合的程度，评估模型拟合的好坏，在线性回归中，损失函数通常为样本输出和假设函数的差取平方 
凸函数？

给公式的同时，应该给具体的例子，2维或3维的几何示意。

梯度负方向
拟合函数

1. 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。
2.特征（feature）：指的是样本中输入部分，比如样本（x0,y0）,（x1,y1）,则样本特征为x，样本输出为y。
3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)。比如对于样本（xi,yi）(i=1,2,...n),可以采用拟合函数如下： hθ(x) = θ0+θ1x。
4. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),采用线性回归，损失函数为：
J(θ0,θ1)=∑i=1m(hθ(xi)−yi)2J(θ0,θ1)=∑i=1m(hθ(xi)−yi)2
其中xixi表示样本特征x的第i个元素，yiyi表示样本输出y的第i个元素，hθ(xi)hθ(xi)为假设函数。

前位置的损失函数的梯度

矩阵求导  
XX 为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。
矩阵求导链式法则

代数表示 vs 矩阵表示

用Excel理解梯度下降
https://www.leiphone.com/news/201704/lfQ5ff90g9EkqcSi.html 

计算误差（SSE）
    * 误差的平方和

数据归一化
(x-min)/(max-min)

初始化值？